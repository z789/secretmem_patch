diff -Nurp linux-5.4.0.89.orig/arch/arm64/include/asm/cacheflush.h linux-5.4.0.89/arch/arm64/include/asm/cacheflush.h
--- linux-5.4.0.89.orig/arch/arm64/include/asm/cacheflush.h	2021-10-16 10:14:46.000000000 +0800
+++ linux-5.4.0.89/arch/arm64/include/asm/cacheflush.h	2021-10-15 23:12:24.102443564 +0800
@@ -174,9 +174,4 @@ static inline void flush_cache_vunmap(un
 {
 }
 
-int set_memory_valid(unsigned long addr, int numpages, int enable);
-
-int set_direct_map_invalid_noflush(struct page *page);
-int set_direct_map_default_noflush(struct page *page);
-
 #endif
diff -Nurp linux-5.4.0.89.orig/arch/arm64/include/asm/Kbuild linux-5.4.0.89/arch/arm64/include/asm/Kbuild
--- linux-5.4.0.89.orig/arch/arm64/include/asm/Kbuild	2019-11-25 08:32:01.000000000 +0800
+++ linux-5.4.0.89/arch/arm64/include/asm/Kbuild	2021-10-15 23:12:24.102443564 +0800
@@ -20,7 +20,6 @@ generic-y += msi.h
 generic-y += qrwlock.h
 generic-y += qspinlock.h
 generic-y += serial.h
-generic-y += set_memory.h
 generic-y += switch_to.h
 generic-y += trace_clock.h
 generic-y += unaligned.h
diff -Nurp linux-5.4.0.89.orig/arch/arm64/include/asm/set_memory.h linux-5.4.0.89/arch/arm64/include/asm/set_memory.h
--- linux-5.4.0.89.orig/arch/arm64/include/asm/set_memory.h	1970-01-01 08:00:00.000000000 +0800
+++ linux-5.4.0.89/arch/arm64/include/asm/set_memory.h	2021-10-15 23:12:24.102443564 +0800
@@ -0,0 +1,17 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+
+#ifndef _ASM_ARM64_SET_MEMORY_H
+#define _ASM_ARM64_SET_MEMORY_H
+
+#include <asm-generic/set_memory.h>
+
+bool can_set_direct_map(void);
+#define can_set_direct_map can_set_direct_map
+
+int set_memory_valid(unsigned long addr, int numpages, int enable);
+
+int set_direct_map_invalid_noflush(struct page *page);
+int set_direct_map_default_noflush(struct page *page);
+bool kernel_page_present(struct page *page);
+
+#endif /* _ASM_ARM64_SET_MEMORY_H */
diff -Nurp linux-5.4.0.89.orig/arch/arm64/kernel/machine_kexec.c linux-5.4.0.89/arch/arm64/kernel/machine_kexec.c
--- linux-5.4.0.89.orig/arch/arm64/kernel/machine_kexec.c	2021-10-16 10:14:46.000000000 +0800
+++ linux-5.4.0.89/arch/arm64/kernel/machine_kexec.c	2021-10-15 23:12:24.102443564 +0800
@@ -11,6 +11,7 @@
 #include <linux/kernel.h>
 #include <linux/kexec.h>
 #include <linux/page-flags.h>
+#include <linux/set_memory.h>
 #include <linux/smp.h>
 
 #include <asm/cacheflush.h>
diff -Nurp linux-5.4.0.89.orig/arch/arm64/mm/mmu.c linux-5.4.0.89/arch/arm64/mm/mmu.c
--- linux-5.4.0.89.orig/arch/arm64/mm/mmu.c	2021-10-16 10:14:46.000000000 +0800
+++ linux-5.4.0.89/arch/arm64/mm/mmu.c	2021-10-15 23:12:24.102443564 +0800
@@ -21,6 +21,7 @@
 #include <linux/io.h>
 #include <linux/mm.h>
 #include <linux/vmalloc.h>
+#include <linux/set_memory.h>
 
 #include <asm/barrier.h>
 #include <asm/cputype.h>
@@ -463,7 +464,7 @@ static void __init map_mem(pgd_t *pgdp)
 	struct memblock_region *reg;
 	int flags = 0;
 
-	if (rodata_full || debug_pagealloc_enabled())
+	if (can_set_direct_map() || crash_mem_map)
 		flags = NO_BLOCK_MAPPINGS | NO_CONT_MAPPINGS;
 
 	/*
@@ -1055,7 +1056,7 @@ int arch_add_memory(int nid, u64 start,
 {
 	int flags = 0;
 
-	if (rodata_full || debug_pagealloc_enabled())
+	if (can_set_direct_map() || IS_ENABLED(CONFIG_KFENCE))
 		flags = NO_BLOCK_MAPPINGS | NO_CONT_MAPPINGS;
 
 	__create_pgd_mapping(swapper_pg_dir, start, __phys_to_virt(start),
diff -Nurp linux-5.4.0.89.orig/arch/arm64/mm/pageattr.c linux-5.4.0.89/arch/arm64/mm/pageattr.c
--- linux-5.4.0.89.orig/arch/arm64/mm/pageattr.c	2019-11-25 08:32:01.000000000 +0800
+++ linux-5.4.0.89/arch/arm64/mm/pageattr.c	2021-10-15 23:12:24.102443564 +0800
@@ -19,6 +19,11 @@ struct page_change_data {
 
 bool rodata_full __ro_after_init = IS_ENABLED(CONFIG_RODATA_FULL_DEFAULT_ENABLED);
 
+bool can_set_direct_map(void)
+{
+	return rodata_full || debug_pagealloc_enabled();
+}
+
 static int change_page_range(pte_t *ptep, unsigned long addr, void *data)
 {
 	struct page_change_data *cdata = data;
@@ -155,7 +160,7 @@ int set_direct_map_invalid_noflush(struc
 		.clear_mask = __pgprot(PTE_VALID),
 	};
 
-	if (!rodata_full)
+	if (!can_set_direct_map())
 		return 0;
 
 	return apply_to_page_range(&init_mm,
@@ -170,7 +175,7 @@ int set_direct_map_default_noflush(struc
 		.clear_mask = __pgprot(PTE_RDONLY),
 	};
 
-	if (!rodata_full)
+	if (!can_set_direct_map())
 		return 0;
 
 	return apply_to_page_range(&init_mm,
@@ -180,7 +185,7 @@ int set_direct_map_default_noflush(struc
 
 void __kernel_map_pages(struct page *page, int numpages, int enable)
 {
-	if (!debug_pagealloc_enabled() && !rodata_full)
+	if (!can_set_direct_map())
 		return;
 
 	set_memory_valid((unsigned long)page_address(page), numpages, enable);
@@ -203,7 +208,7 @@ bool kernel_page_present(struct page *pa
 	pte_t *ptep;
 	unsigned long addr = (unsigned long)page_address(page);
 
-	if (!debug_pagealloc_enabled() && !rodata_full)
+	if (!can_set_direct_map())
 		return true;
 
 	pgdp = pgd_offset_k(addr);
diff -Nurp linux-5.4.0.89.orig/include/linux/memfd.h linux-5.4.0.89/include/linux/memfd.h
--- linux-5.4.0.89.orig/include/linux/memfd.h	2019-11-25 08:32:01.000000000 +0800
+++ linux-5.4.0.89/include/linux/memfd.h	2021-10-15 23:15:29.928483509 +0800
@@ -13,4 +13,13 @@ static inline long memfd_fcntl(struct fi
 }
 #endif
 
+#ifdef CONFIG_SECRETMEM
+extern int sys_memfd_secret(unsigned int flags);
+#else
+static int sys_memfd_secret(unsigned int flags)
+{
+	return -EINVAL;
+}
+#endif
+
 #endif /* __LINUX_MEMFD_H */
diff -Nurp linux-5.4.0.89.orig/include/linux/secretmem.h linux-5.4.0.89/include/linux/secretmem.h
--- linux-5.4.0.89.orig/include/linux/secretmem.h	1970-01-01 08:00:00.000000000 +0800
+++ linux-5.4.0.89/include/linux/secretmem.h	2021-10-15 23:12:24.130459278 +0800
@@ -0,0 +1,54 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+#ifndef _LINUX_SECRETMEM_H
+#define _LINUX_SECRETMEM_H
+
+#ifdef CONFIG_SECRETMEM
+
+extern const struct address_space_operations secretmem_aops;
+
+static inline bool page_is_secretmem(struct page *page)
+{
+	struct address_space *mapping;
+
+	/*
+	 * Using page_mapping() is quite slow because of the actual call
+	 * instruction and repeated compound_head(page) inside the
+	 * page_mapping() function.
+	 * We know that secretmem pages are not compound and LRU so we can
+	 * save a couple of cycles here.
+	 */
+	if (PageCompound(page) || !PageLRU(page))
+		return false;
+
+	mapping = (struct address_space *)
+		((unsigned long)page->mapping & ~PAGE_MAPPING_FLAGS);
+
+	if (mapping != page->mapping)
+		return false;
+
+	return mapping->a_ops == &secretmem_aops;
+}
+
+bool vma_is_secretmem(struct vm_area_struct *vma);
+bool secretmem_active(void);
+
+#else
+
+static inline bool vma_is_secretmem(struct vm_area_struct *vma)
+{
+	return false;
+}
+
+static inline bool page_is_secretmem(struct page *page)
+{
+	return false;
+}
+ 
+static inline bool secretmem_active(void)
+{
+	return false;
+}
+
+#endif /* CONFIG_SECRETMEM */
+
+#endif /* _LINUX_SECRETMEM_H */
diff -Nurp linux-5.4.0.89.orig/include/linux/set_memory.h linux-5.4.0.89/include/linux/set_memory.h
--- linux-5.4.0.89.orig/include/linux/set_memory.h	2021-10-16 10:14:46.000000000 +0800
+++ linux-5.4.0.89/include/linux/set_memory.h	2021-10-15 23:12:24.130459278 +0800
@@ -23,7 +23,23 @@ static inline int set_direct_map_default
 {
 	return 0;
 }
+static inline bool kernel_page_present(struct page *page)
+{
+        return true;
+}
+#else /* CONFIG_ARCH_HAS_SET_DIRECT_MAP */
+/*
+ * Some architectures, e.g. ARM64 can disable direct map modifications at
+ * boot time. Let them overrive this query.
+ */
+#ifndef can_set_direct_map
+static inline bool can_set_direct_map(void)
+{
+        return true;
+}
+#define can_set_direct_map can_set_direct_map
 #endif
+#endif /* CONFIG_ARCH_HAS_SET_DIRECT_MAP */
 
 #ifndef set_mce_nospec
 static inline int set_mce_nospec(unsigned long pfn, bool unmap)
diff -Nurp linux-5.4.0.89.orig/include/uapi/linux/magic.h linux-5.4.0.89/include/uapi/linux/magic.h
--- linux-5.4.0.89.orig/include/uapi/linux/magic.h	2021-10-16 10:14:46.000000000 +0800
+++ linux-5.4.0.89/include/uapi/linux/magic.h	2021-10-15 23:12:24.134461523 +0800
@@ -95,6 +95,8 @@
 #define DMA_BUF_MAGIC		0x444d4142	/* "DMAB" */
 #define DEVMEM_MAGIC		0x454d444d	/* "DMEM" */
 #define Z3FOLD_MAGIC		0x33
+#define PPC_CMM_MAGIC		0xc7571590
+#define SECRETMEM_MAGIC		0x5345434d	/* "SECM" */
 
 #define SHIFTFS_MAGIC		0x6a656a62
 
diff -Nurp linux-5.4.0.89.orig/include/uapi/linux/memfd.h linux-5.4.0.89/include/uapi/linux/memfd.h
--- linux-5.4.0.89.orig/include/uapi/linux/memfd.h	2019-11-25 08:32:01.000000000 +0800
+++ linux-5.4.0.89/include/uapi/linux/memfd.h	2021-10-15 23:16:05.611702838 +0800
@@ -8,6 +8,7 @@
 #define MFD_CLOEXEC		0x0001U
 #define MFD_ALLOW_SEALING	0x0002U
 #define MFD_HUGETLB		0x0004U
+#define MFD_SECRET		0x0008U
 
 /*
  * Huge page size encoding when MFD_HUGETLB is specified, and a huge page
diff -Nurp linux-5.4.0.89.orig/kernel/power/hibernate.c linux-5.4.0.89/kernel/power/hibernate.c
--- linux-5.4.0.89.orig/kernel/power/hibernate.c	2021-10-16 10:14:46.000000000 +0800
+++ linux-5.4.0.89/kernel/power/hibernate.c	2021-10-15 23:12:24.134461523 +0800
@@ -31,6 +31,7 @@
 #include <linux/genhd.h>
 #include <linux/ktime.h>
 #include <linux/security.h>
+#include <linux/secretmem.h>
 #include <trace/events/power.h>
 
 #include "power.h"
@@ -69,7 +70,9 @@ static const struct platform_hibernation
 
 bool hibernation_available(void)
 {
-	return nohibernate == 0 && !security_locked_down(LOCKDOWN_HIBERNATION);
+	return nohibernate == 0 &&
+		!security_locked_down(LOCKDOWN_HIBERNATION) &&
+		!secretmem_active();
 }
 
 /**
diff -Nurp linux-5.4.0.89.orig/mm/gup.c linux-5.4.0.89/mm/gup.c
--- linux-5.4.0.89.orig/mm/gup.c	2021-10-16 10:14:46.000000000 +0800
+++ linux-5.4.0.89/mm/gup.c	2021-10-15 23:12:24.134461523 +0800
@@ -10,6 +10,7 @@
 #include <linux/rmap.h>
 #include <linux/swap.h>
 #include <linux/swapops.h>
+#include <linux/secretmem.h>
 
 #include <linux/sched/signal.h>
 #include <linux/rwsem.h>
@@ -570,6 +571,9 @@ struct page *follow_page(struct vm_area_
 	struct follow_page_context ctx = { NULL };
 	struct page *page;
 
+	if (vma_is_secretmem(vma))
+		return NULL;
+
 	page = follow_page_mask(vma, address, foll_flags, &ctx);
 	if (ctx.pgmap)
 		put_dev_pagemap(ctx.pgmap);
@@ -704,6 +708,12 @@ static int check_vma_flags(struct vm_are
 	if (gup_flags & FOLL_ANON && !vma_is_anonymous(vma))
 		return -EFAULT;
 
+	if ((gup_flags & FOLL_LONGTERM) && vma_is_fsdax(vma))
+                return -EOPNOTSUPP;
+
+	if (vma_is_secretmem(vma))
+		return -EFAULT;
+
 	if (write) {
 		if (!(vm_flags & VM_WRITE)) {
 			if (!(gup_flags & FOLL_FORCE))
@@ -1874,6 +1884,11 @@ static int gup_pte_range(pmd_t pmd, unsi
 		if (!head)
 			goto pte_unmap;
 
+                if (unlikely(page_is_secretmem(page))) {
+			put_page(head);
+                        goto pte_unmap;
+                }
+
 		if (unlikely(pte_val(pte) != pte_val(*ptep))) {
 			put_page(head);
 			goto pte_unmap;
diff -Nurp linux-5.4.0.89.orig/mm/internal.h linux-5.4.0.89/mm/internal.h
--- linux-5.4.0.89.orig/mm/internal.h	2021-10-16 10:14:46.000000000 +0800
+++ linux-5.4.0.89/mm/internal.h	2021-10-15 23:12:24.134461523 +0800
@@ -307,6 +307,9 @@ static inline void munlock_vma_pages_all
  */
 extern void mlock_vma_page(struct page *page);
 extern unsigned int munlock_vma_page(struct page *page);
+ 
+extern int mlock_future_check(struct mm_struct *mm, unsigned long flags,
+			      unsigned long len);
 
 /*
  * Clear the page's PageMlocked().  This can be useful in a situation where
diff -Nurp linux-5.4.0.89.orig/mm/Kconfig linux-5.4.0.89/mm/Kconfig
--- linux-5.4.0.89.orig/mm/Kconfig	2021-10-16 10:14:46.000000000 +0800
+++ linux-5.4.0.89/mm/Kconfig	2021-10-15 23:12:24.134461523 +0800
@@ -723,4 +723,8 @@ config ARCH_HAS_PTE_SPECIAL
 config ARCH_HAS_HUGEPD
 	bool
 
+config SECRETMEM
+	def_bool ARCH_HAS_SET_DIRECT_MAP && !EMBEDDED
+	select STRICT_DEVMEM
+
 endmenu
diff -Nurp linux-5.4.0.89.orig/mm/Makefile linux-5.4.0.89/mm/Makefile
--- linux-5.4.0.89.orig/mm/Makefile	2021-10-16 10:14:46.000000000 +0800
+++ linux-5.4.0.89/mm/Makefile	2021-10-15 23:12:24.134461523 +0800
@@ -107,3 +107,4 @@ obj-$(CONFIG_PERCPU_STATS) += percpu-sta
 obj-$(CONFIG_ZONE_DEVICE) += memremap.o
 obj-$(CONFIG_HMM_MIRROR) += hmm.o
 obj-$(CONFIG_MEMFD_CREATE) += memfd.o
+obj-$(CONFIG_SECRETMEM) += secretmem.o
diff -Nurp linux-5.4.0.89.orig/mm/memfd.c linux-5.4.0.89/mm/memfd.c
--- linux-5.4.0.89.orig/mm/memfd.c	2019-11-25 08:32:01.000000000 +0800
+++ linux-5.4.0.89/mm/memfd.c	2021-10-15 23:39:51.828969585 +0800
@@ -245,7 +245,8 @@ long memfd_fcntl(struct file *file, unsi
 #define MFD_NAME_PREFIX_LEN (sizeof(MFD_NAME_PREFIX) - 1)
 #define MFD_NAME_MAX_LEN (NAME_MAX - MFD_NAME_PREFIX_LEN)
 
-#define MFD_ALL_FLAGS (MFD_CLOEXEC | MFD_ALLOW_SEALING | MFD_HUGETLB)
+#define MFD_SECRET_MASK (MFD_CLOEXEC | MFD_SECRET)
+#define MFD_ALL_FLAGS (MFD_CLOEXEC | MFD_ALLOW_SEALING | MFD_HUGETLB |MFD_SECRET)
 
 SYSCALL_DEFINE2(memfd_create,
 		const char __user *, uname,
@@ -257,6 +258,11 @@ SYSCALL_DEFINE2(memfd_create,
 	char *name;
 	long len;
 
+	if (flags & MFD_SECRET) {
+		if (flags & ~(unsigned int)MFD_SECRET_MASK)
+			return -EINVAL;
+	}
+
 	if (!(flags & MFD_HUGETLB)) {
 		if (flags & ~(unsigned int)MFD_ALL_FLAGS)
 			return -EINVAL;
@@ -267,6 +273,9 @@ SYSCALL_DEFINE2(memfd_create,
 			return -EINVAL;
 	}
 
+	if (flags & MFD_SECRET)
+		return sys_memfd_secret((flags & MFD_CLOEXEC) ? O_CLOEXEC : 0);
+
 	/* length includes terminating zero */
 	len = strnlen_user(uname, MFD_NAME_MAX_LEN + 1);
 	if (len <= 0)
diff -Nurp linux-5.4.0.89.orig/mm/mlock.c linux-5.4.0.89/mm/mlock.c
--- linux-5.4.0.89.orig/mm/mlock.c	2019-11-25 08:32:01.000000000 +0800
+++ linux-5.4.0.89/mm/mlock.c	2021-10-15 23:12:24.134461523 +0800
@@ -23,6 +23,7 @@
 #include <linux/hugetlb.h>
 #include <linux/memcontrol.h>
 #include <linux/mm_inline.h>
+#include <linux/secretmem.h>
 
 #include "internal.h"
 
@@ -528,7 +529,7 @@ static int mlock_fixup(struct vm_area_st
 
 	if (newflags == vma->vm_flags || (vma->vm_flags & VM_SPECIAL) ||
 	    is_vm_hugetlb_page(vma) || vma == get_gate_vma(current->mm) ||
-	    vma_is_dax(vma))
+	    vma_is_dax(vma) || vma_is_secretmem(vma))
 		/* don't set VM_LOCKED or VM_LOCKONFAULT and don't count */
 		goto out;
 
diff -Nurp linux-5.4.0.89.orig/mm/mmap.c linux-5.4.0.89/mm/mmap.c
--- linux-5.4.0.89.orig/mm/mmap.c	2021-10-16 10:14:46.000000000 +0800
+++ linux-5.4.0.89/mm/mmap.c	2021-10-15 23:12:24.134461523 +0800
@@ -1333,7 +1333,7 @@ static inline unsigned long round_hint_t
 	return hint;
 }
 
-static inline int mlock_future_check(struct mm_struct *mm,
+int mlock_future_check(struct mm_struct *mm,
 				     unsigned long flags,
 				     unsigned long len)
 {
diff -Nurp linux-5.4.0.89.orig/mm/secretmem.c linux-5.4.0.89/mm/secretmem.c
--- linux-5.4.0.89.orig/mm/secretmem.c	1970-01-01 08:00:00.000000000 +0800
+++ linux-5.4.0.89/mm/secretmem.c	2021-10-15 23:40:21.759196210 +0800
@@ -0,0 +1,263 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright IBM Corporation, 2021
+ *
+ * Author: Mike Rapoport <rppt@linux.ibm.com>
+ */
+
+#include <linux/mm.h>
+#include <linux/fs.h>
+#include <linux/swap.h>
+#include <linux/mount.h>
+#include <linux/memfd.h>
+#include <linux/bitops.h>
+#include <linux/printk.h>
+#include <linux/pagemap.h>
+#include <linux/syscalls.h>
+#include <linux/pseudo_fs.h>
+#include <linux/secretmem.h>
+#include <linux/set_memory.h>
+#include <linux/sched/signal.h>
+
+#include <uapi/linux/magic.h>
+
+#include <asm/tlbflush.h>
+
+#include "internal.h"
+
+#undef pr_fmt
+#define pr_fmt(fmt) "secretmem: " fmt
+
+/*
+ * Define mode and flag masks to allow validation of the system call
+ * parameters.
+ */
+#define SECRETMEM_MODE_MASK	(0x0)
+#define SECRETMEM_FLAGS_MASK	SECRETMEM_MODE_MASK
+
+static bool secretmem_enable __ro_after_init;
+module_param_named(enable, secretmem_enable, bool, 0400);
+MODULE_PARM_DESC(secretmem_enable,
+		 "Enable secretmem and memfd_secret(2) system call");
+
+static int __init secretmem_enabled_setup(char *str)
+{
+        unsigned long enabled;
+        if (!kstrtoul(str, 0, &enabled))
+                secretmem_enable = enabled ? 1 : 0;
+        return 1;
+}
+__setup("secretmem=", secretmem_enabled_setup);
+
+static atomic_t secretmem_users;
+
+bool secretmem_active(void)
+{
+	return !!atomic_read(&secretmem_users);
+}
+
+static vm_fault_t secretmem_fault(struct vm_fault *vmf)
+{
+	struct address_space *mapping = vmf->vma->vm_file->f_mapping;
+	struct inode *inode = file_inode(vmf->vma->vm_file);
+	pgoff_t offset = vmf->pgoff;
+	gfp_t gfp = vmf->gfp_mask;
+	unsigned long addr;
+	struct page *page;
+	int err;
+
+	if (((loff_t)vmf->pgoff << PAGE_SHIFT) >= i_size_read(inode))
+		return vmf_error(-EINVAL);
+
+retry:
+	page = find_lock_page(mapping, offset);
+	if (!page) {
+		page = alloc_page(gfp | __GFP_ZERO);
+		if (!page)
+			return VM_FAULT_OOM;
+
+		err = set_direct_map_invalid_noflush(page);
+		if (err) {
+			put_page(page);
+			return vmf_error(err);
+		}
+
+		__SetPageUptodate(page);
+		err = add_to_page_cache_lru(page, mapping, offset, gfp);
+		if (unlikely(err)) {
+			put_page(page);
+			/*
+			 * If a split of large page was required, it
+			 * already happened when we marked the page invalid
+			 * which guarantees that this call won't fail
+			 */
+			set_direct_map_default_noflush(page);
+			if (err == -EEXIST)
+				goto retry;
+
+			return vmf_error(err);
+		}
+
+		addr = (unsigned long)page_address(page);
+		flush_tlb_kernel_range(addr, addr + PAGE_SIZE);
+	}
+
+	vmf->page = page;
+	return VM_FAULT_LOCKED;
+}
+
+static const struct vm_operations_struct secretmem_vm_ops = {
+	.fault = secretmem_fault,
+};
+
+static int secretmem_release(struct inode *inode, struct file *file)
+{
+	atomic_dec(&secretmem_users);
+	return 0;
+}
+
+static int secretmem_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	unsigned long len = vma->vm_end - vma->vm_start;
+
+	if ((vma->vm_flags & (VM_SHARED | VM_MAYSHARE)) == 0)
+		return -EINVAL;
+
+	if (mlock_future_check(vma->vm_mm, vma->vm_flags | VM_LOCKED, len))
+		return -EAGAIN;
+
+	vma->vm_flags |= VM_LOCKED | VM_DONTDUMP;
+	vma->vm_ops = &secretmem_vm_ops;
+
+	return 0;
+}
+
+bool vma_is_secretmem(struct vm_area_struct *vma)
+{
+	return vma->vm_ops == &secretmem_vm_ops;
+}
+
+static const struct file_operations secretmem_fops = {
+	.release	= secretmem_release,
+	.mmap		= secretmem_mmap,
+};
+
+static bool secretmem_isolate_page(struct page *page, isolate_mode_t mode)
+{
+	return false;
+}
+
+static int secretmem_migratepage(struct address_space *mapping,
+				 struct page *newpage, struct page *page,
+				 enum migrate_mode mode)
+{
+	return -EBUSY;
+}
+
+static void secretmem_freepage(struct page *page)
+{
+	set_direct_map_default_noflush(page);
+	clear_highpage(page);
+}
+
+const struct address_space_operations secretmem_aops = {
+	.freepage	= secretmem_freepage,
+	.migratepage	= secretmem_migratepage,
+	.isolate_page	= secretmem_isolate_page,
+};
+
+static struct vfsmount *secretmem_mnt;
+
+static struct file *secretmem_file_create(unsigned long flags)
+{
+	struct file *file = ERR_PTR(-ENOMEM);
+	struct inode *inode;
+
+	inode = alloc_anon_inode(secretmem_mnt->mnt_sb);
+	if (IS_ERR(inode))
+		return ERR_CAST(inode);
+
+	file = alloc_file_pseudo(inode, secretmem_mnt, "secretmem",
+				 O_RDWR, &secretmem_fops);
+	if (IS_ERR(file))
+		goto err_free_inode;
+
+	mapping_set_gfp_mask(inode->i_mapping, GFP_HIGHUSER);
+	mapping_set_unevictable(inode->i_mapping);
+
+	inode->i_mapping->a_ops = &secretmem_aops;
+
+	/* pretend we are a normal file with zero size */
+	inode->i_mode |= S_IFREG;
+	inode->i_size = 0;
+
+	return file;
+
+err_free_inode:
+	iput(inode);
+	return file;
+}
+
+int sys_memfd_secret(unsigned int flags)
+{
+	struct file *file;
+	int fd, err;
+
+	/* make sure local flags do not confict with global fcntl.h */
+	BUILD_BUG_ON(SECRETMEM_FLAGS_MASK & O_CLOEXEC);
+
+	if (!secretmem_enable)
+		return -ENOSYS;
+
+	if (flags & ~(SECRETMEM_FLAGS_MASK | O_CLOEXEC))
+		return -EINVAL;
+
+	fd = get_unused_fd_flags(flags & O_CLOEXEC);
+	if (fd < 0)
+		return fd;
+
+	file = secretmem_file_create(flags);
+	if (IS_ERR(file)) {
+		err = PTR_ERR(file);
+		goto err_put_fd;
+	}
+
+	file->f_flags |= O_LARGEFILE;
+
+	fd_install(fd, file);
+	atomic_inc(&secretmem_users);
+	return fd;
+
+err_put_fd:
+	put_unused_fd(fd);
+	return err;
+}
+
+static int secretmem_init_fs_context(struct fs_context *fc)
+{
+	return init_pseudo(fc, SECRETMEM_MAGIC) ? 0 : -ENOMEM;
+}
+
+static struct file_system_type secretmem_fs = {
+	.name		= "secretmem",
+	.init_fs_context = secretmem_init_fs_context,
+	.kill_sb	= kill_anon_super,
+};
+
+static int secretmem_init(void)
+{
+	int ret = 0;
+
+	if (!secretmem_enable)
+		return ret;
+
+	secretmem_mnt = kern_mount(&secretmem_fs);
+	if (IS_ERR(secretmem_mnt))
+		ret = PTR_ERR(secretmem_mnt);
+
+	/* prevent secretmem mappings from ever getting PROT_EXEC */
+	secretmem_mnt->mnt_flags |= MNT_NOEXEC;
+
+	return ret;
+}
+fs_initcall(secretmem_init);
diff -Nurp linux-5.4.0.89.orig/tools/testing/selftests/kselftest.h linux-5.4.0.89/tools/testing/selftests/kselftest.h
--- linux-5.4.0.89.orig/tools/testing/selftests/kselftest.h	2019-11-25 08:32:01.000000000 +0800
+++ linux-5.4.0.89/tools/testing/selftests/kselftest.h	2021-10-15 23:12:24.138463768 +0800
@@ -161,6 +161,18 @@ static inline int ksft_exit_fail(void)
 	exit(KSFT_FAIL);
 }
 
+/**
+ * ksft_exit() - Exit selftest based on truth of condition
+ *
+ * @condition: if true, exit self test with success, otherwise fail.
+ */
+#define ksft_exit(condition) do {       \
+        if (!!(condition))              \
+                ksft_exit_pass();       \
+        else                            \
+                ksft_exit_fail();       \
+        } while (0)
+
 static inline int ksft_exit_fail_msg(const char *msg, ...)
 {
 	int saved_errno = errno;
diff -Nurp linux-5.4.0.89.orig/tools/testing/selftests/vm/.gitignore linux-5.4.0.89/tools/testing/selftests/vm/.gitignore
--- linux-5.4.0.89.orig/tools/testing/selftests/vm/.gitignore	2019-11-25 08:32:01.000000000 +0800
+++ linux-5.4.0.89/tools/testing/selftests/vm/.gitignore	2021-10-15 23:12:24.138463768 +0800
@@ -14,3 +14,4 @@ virtual_address_range
 gup_benchmark
 va_128TBswitch
 map_fixed_noreplace
+memfd_secret
diff -Nurp linux-5.4.0.89.orig/tools/testing/selftests/vm/Makefile linux-5.4.0.89/tools/testing/selftests/vm/Makefile
--- linux-5.4.0.89.orig/tools/testing/selftests/vm/Makefile	2019-11-25 08:32:01.000000000 +0800
+++ linux-5.4.0.89/tools/testing/selftests/vm/Makefile	2021-10-15 23:12:24.138463768 +0800
@@ -10,6 +10,7 @@ TEST_GEN_FILES += hugepage-shm
 TEST_GEN_FILES += map_hugetlb
 TEST_GEN_FILES += map_fixed_noreplace
 TEST_GEN_FILES += map_populate
+TEST_GEN_FILES += memfd_secret
 TEST_GEN_FILES += mlock-random-test
 TEST_GEN_FILES += mlock2-tests
 TEST_GEN_FILES += on-fault-limit
@@ -28,4 +29,4 @@ include ../lib.mk
 
 $(OUTPUT)/userfaultfd: LDLIBS += -lpthread
 
-$(OUTPUT)/mlock-random-test: LDLIBS += -lcap
+$(OUTPUT)/mlock-random-test $(OUTPUT)/memfd_secret: LDLIBS += -lcap
diff -Nurp linux-5.4.0.89.orig/tools/testing/selftests/vm/memfd_secret.c linux-5.4.0.89/tools/testing/selftests/vm/memfd_secret.c
--- linux-5.4.0.89.orig/tools/testing/selftests/vm/memfd_secret.c	1970-01-01 08:00:00.000000000 +0800
+++ linux-5.4.0.89/tools/testing/selftests/vm/memfd_secret.c	2021-10-15 23:30:07.242458044 +0800
@@ -0,0 +1,305 @@
+// SPDX-License-Identifier: GPL-2.0
+/*
+ * Copyright IBM Corporation, 2021
+ *
+ * Author: Mike Rapoport <rppt@linux.ibm.com>
+ */
+
+#define _GNU_SOURCE
+#include <sys/uio.h>
+#include <sys/mman.h>
+#include <sys/wait.h>
+#include <sys/types.h>
+#include <sys/ptrace.h>
+#include <sys/syscall.h>
+#include <sys/resource.h>
+#include <sys/capability.h>
+
+#include <stdlib.h>
+#include <string.h>
+#include <unistd.h>
+#include <errno.h>
+#include <stdio.h>
+
+#include "../kselftest.h"
+
+#define fail(fmt, ...) ksft_test_result_fail(fmt, ##__VA_ARGS__)
+#define pass(fmt, ...) ksft_test_result_pass(fmt, ##__VA_ARGS__)
+#define skip(fmt, ...) ksft_test_result_skip(fmt, ##__VA_ARGS__)
+
+//#ifdef __NR_memfd_secret
+
+#define PATTERN	0x55
+
+static const int prot = PROT_READ | PROT_WRITE;
+static const int mode = MAP_SHARED;
+
+static unsigned long page_size;
+static unsigned long mlock_limit_cur;
+static unsigned long mlock_limit_max;
+
+#ifdef __NR_memfd_secret
+static int memfd_secret(unsigned int flags)
+{
+	return syscall(__NR_memfd_secret, flags);
+}
+#else
+static int memfd_secret(unsigned int flags)
+{
+	return syscall(__NR_memfd_create, "secretmem", MFD_SECRET | flags);
+}
+#endif
+
+static void test_file_apis(int fd)
+{
+	char buf[64];
+
+	if ((read(fd, buf, sizeof(buf)) >= 0) ||
+	    (write(fd, buf, sizeof(buf)) >= 0) ||
+	    (pread(fd, buf, sizeof(buf), 0) >= 0) ||
+	    (pwrite(fd, buf, sizeof(buf), 0) >= 0))
+		fail("unexpected file IO\n");
+	else
+		pass("file IO is blocked as expected\n");
+}
+
+static void test_mlock_limit(int fd)
+{
+	size_t len;
+	char *mem;
+
+	len = mlock_limit_cur;
+	mem = mmap(NULL, len, prot, mode, fd, 0);
+	if (mem == MAP_FAILED) {
+		fail("unable to mmap secret memory\n");
+		return;
+	}
+	munmap(mem, len);
+
+	len = mlock_limit_max * 2;
+	mem = mmap(NULL, len, prot, mode, fd, 0);
+	if (mem != MAP_FAILED) {
+		fail("unexpected mlock limit violation\n");
+		munmap(mem, len);
+		return;
+	}
+
+	pass("mlock limit is respected\n");
+}
+
+static void try_process_vm_read(int fd, int pipefd[2])
+{
+	struct iovec liov, riov;
+	char buf[64];
+	char *mem;
+
+	if (read(pipefd[0], &mem, sizeof(mem)) < 0) {
+		fail("pipe write: %s\n", strerror(errno));
+		exit(KSFT_FAIL);
+	}
+
+	liov.iov_len = riov.iov_len = sizeof(buf);
+	liov.iov_base = buf;
+	riov.iov_base = mem;
+
+	if (process_vm_readv(getppid(), &liov, 1, &riov, 1, 0) < 0) {
+		if (errno == ENOSYS)
+			exit(KSFT_SKIP);
+		exit(KSFT_PASS);
+	}
+
+	exit(KSFT_FAIL);
+}
+
+static void try_ptrace(int fd, int pipefd[2])
+{
+	pid_t ppid = getppid();
+	int status;
+	char *mem;
+	long ret;
+
+	if (read(pipefd[0], &mem, sizeof(mem)) < 0) {
+		perror("pipe write");
+		exit(KSFT_FAIL);
+	}
+
+	ret = ptrace(PTRACE_ATTACH, ppid, 0, 0);
+	if (ret) {
+		perror("ptrace_attach");
+		exit(KSFT_FAIL);
+	}
+
+	ret = waitpid(ppid, &status, WUNTRACED);
+	if ((ret != ppid) || !(WIFSTOPPED(status))) {
+		fprintf(stderr, "weird waitppid result %ld stat %x\n",
+			ret, status);
+		exit(KSFT_FAIL);
+	}
+
+	if (ptrace(PTRACE_PEEKDATA, ppid, mem, 0))
+		exit(KSFT_PASS);
+
+	exit(KSFT_FAIL);
+}
+
+static void check_child_status(pid_t pid, const char *name)
+{
+	int status;
+
+	waitpid(pid, &status, 0);
+
+	if (WIFEXITED(status) && WEXITSTATUS(status) == KSFT_SKIP) {
+		skip("%s is not supported\n", name);
+		return;
+	}
+
+	if ((WIFEXITED(status) && WEXITSTATUS(status) == KSFT_PASS) ||
+	    WIFSIGNALED(status)) {
+		pass("%s is blocked as expected\n", name);
+		return;
+	}
+
+	fail("%s: unexpected memory access\n", name);
+}
+
+static void test_remote_access(int fd, const char *name,
+			       void (*func)(int fd, int pipefd[2]))
+{
+	int pipefd[2];
+	pid_t pid;
+	char *mem;
+
+	if (pipe(pipefd)) {
+		fail("pipe failed: %s\n", strerror(errno));
+		return;
+	}
+
+	pid = fork();
+	if (pid < 0) {
+		fail("fork failed: %s\n", strerror(errno));
+		return;
+	}
+
+	if (pid == 0) {
+		func(fd, pipefd);
+		return;
+	}
+
+	mem = mmap(NULL, page_size, prot, mode, fd, 0);
+	if (mem == MAP_FAILED) {
+		fail("Unable to mmap secret memory\n");
+		return;
+	}
+
+	ftruncate(fd, page_size);
+	memset(mem, PATTERN, page_size);
+
+	if (write(pipefd[1], &mem, sizeof(mem)) < 0) {
+		fail("pipe write: %s\n", strerror(errno));
+		return;
+	}
+
+	check_child_status(pid, name);
+}
+
+static void test_process_vm_read(int fd)
+{
+	test_remote_access(fd, "process_vm_read", try_process_vm_read);
+}
+
+static void test_ptrace(int fd)
+{
+	test_remote_access(fd, "ptrace", try_ptrace);
+}
+
+static int set_cap_limits(rlim_t max)
+{
+	struct rlimit new;
+	cap_t cap = cap_init();
+
+	new.rlim_cur = max;
+	new.rlim_max = max;
+	if (setrlimit(RLIMIT_MEMLOCK, &new)) {
+		perror("setrlimit() returns error");
+		return -1;
+	}
+
+	/* drop capabilities including CAP_IPC_LOCK */
+	if (cap_set_proc(cap)) {
+		perror("cap_set_proc() returns error");
+		return -2;
+	}
+
+	return 0;
+}
+
+static void prepare(void)
+{
+	struct rlimit rlim;
+
+	page_size = sysconf(_SC_PAGE_SIZE);
+	if (!page_size)
+		ksft_exit_fail_msg("Failed to get page size %s\n",
+				   strerror(errno));
+
+	if (getrlimit(RLIMIT_MEMLOCK, &rlim))
+		ksft_exit_fail_msg("Unable to detect mlock limit: %s\n",
+				   strerror(errno));
+
+	mlock_limit_cur = rlim.rlim_cur;
+	mlock_limit_max = rlim.rlim_max;
+
+	printf("page_size: %ld, mlock.soft: %ld, mlock.hard: %ld\n",
+	       page_size, mlock_limit_cur, mlock_limit_max);
+
+	if (page_size > mlock_limit_cur)
+		mlock_limit_cur = page_size;
+	if (page_size > mlock_limit_max)
+		mlock_limit_max = page_size;
+
+	if (set_cap_limits(mlock_limit_max))
+		ksft_exit_fail_msg("Unable to set mlock limit: %s\n",
+				   strerror(errno));
+}
+
+#define NUM_TESTS 4
+
+int main(int argc, char *argv[])
+{
+	int fd;
+
+	prepare();
+
+	ksft_print_header();
+	ksft_set_plan(NUM_TESTS);
+
+	fd = memfd_secret(0);
+	if (fd < 0) {
+		if (errno == ENOSYS)
+			ksft_exit_skip("memfd_secret is not supported\n");
+		else
+			ksft_exit_fail_msg("memfd_secret failed: %s\n",
+					   strerror(errno));
+	}
+
+	test_mlock_limit(fd);
+	test_file_apis(fd);
+	test_process_vm_read(fd);
+	test_ptrace(fd);
+
+	close(fd);
+
+	ksft_exit(!ksft_get_fail_cnt());
+}
+
+//#else /* __NR_memfd_secret */
+
+#if 0
+int main(int argc, char *argv[])
+{
+	printf("skip: skipping memfd_secret test (missing __NR_memfd_secret)\n");
+	return KSFT_SKIP;
+}
+#endif
+
+//#endif /* __NR_memfd_secret */
diff -Nurp linux-5.4.0.89.orig/tools/testing/selftests/vm/run_vmtests linux-5.4.0.89/tools/testing/selftests/vm/run_vmtests
--- linux-5.4.0.89.orig/tools/testing/selftests/vm/run_vmtests	2019-11-25 08:32:01.000000000 +0800
+++ linux-5.4.0.89/tools/testing/selftests/vm/run_vmtests	2021-10-15 23:12:24.138463768 +0800
@@ -227,4 +227,21 @@ else
 	exitcode=1
 fi
 
+echo "running memfd_secret test"
+echo "------------------------------------"
+./memfd_secret
+ret_val=$?
+
+if [ $ret_val -eq 0 ]; then
+	echo "[PASS]"
+elif [ $ret_val -eq $ksft_skip ]; then
+	echo "[SKIP]"
+	exitcode=$ksft_skip
+else
+	echo "[FAIL]"
+	exitcode=1
+fi
+
+exit $exitcode
+
 exit $exitcode
